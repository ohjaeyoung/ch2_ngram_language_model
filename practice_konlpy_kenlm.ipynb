{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MIWdWJ8v_Em"
   },
   "source": [
    "# NLP 실습 #3: N-gram Language model\n",
    "황순원의 '소나기' 로 n-gram language model 학습하고 사용해보기\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "* 준비 : konlpy와 kenlm library 설치\n",
    "\n",
    "* Step 1: Data preprocessing - tokenization\n",
    "\n",
    "* Step 2: Language Model 생성\n",
    "\n",
    "* Step 3: Binary file로 Model 변환\n",
    "\n",
    "* Step 4: Model을 활용하여 시퀀스 scoring\n",
    "\n",
    "* Step 5: 주어진 시퀀스에서 다음 형태소 예측\n",
    "\n",
    "* Step 6: 주어진 시퀀스로 시작하는 문장을 예측\n",
    "\n",
    "* Step 7: 다른 말뭉치로 학습한 모델과 비교\n",
    "\n",
    "</br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u58Vf5yByd-j"
   },
   "source": [
    "## 준비: konlpy와 kenlm library 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FFZsFiuSiry6",
    "outputId": "d55b97b8-4ea7-4d54-8c4a-2261693b3107"
   },
   "outputs": [],
   "source": [
    "# 1. konlpy 설치\n",
    "\n",
    "! pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8sMydXeJivy7",
    "outputId": "c60dda78-b1be-496f-c904-6af335c2f2d4"
   },
   "outputs": [],
   "source": [
    "# 2. konlpy의 Kkma 형태소 분석기 import, 형태소 분석 예제\n",
    "\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "tokenizer = Kkma()\n",
    "tokenizer.morphs(\"이 문장이 형태소 단위로 잘 출력되나요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UxOsfjyKLaJ2",
    "outputId": "33d278e2-68bc-478e-eff7-a40ed563d776"
   },
   "outputs": [],
   "source": [
    "# 이전에 nltk 및 'punkt' 모듈을 다운로드 하지 않은 경우에 주석을 풀고 run\n",
    "\n",
    "! pip install nltk\n",
    "\n",
    "import nltk \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LcMwd5ZlLj22",
    "outputId": "b42ae078-67d5-4f55-fe91-abb2ad58000e"
   },
   "outputs": [],
   "source": [
    "# 3. kenlm 설치\n",
    "\n",
    "! wget -O - https://kheafield.com/code/kenlm.tar.gz |tar xz\n",
    "! mkdir ./kenlm/build\n",
    "% cd ./kenlm/build\n",
    "! cmake ..\n",
    "! make -j2\n",
    "! pip install https://github.com/kpu/kenlm/archive/master.zip\n",
    "% cd ../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THyoh5vyyvTE"
   },
   "source": [
    "## Step 1: Data preprocessing - tokenization\n",
    "&emsp;&emsp; sonagi.doc의 내용을 문장 단위로 분리.\n",
    "\n",
    "</br> &emsp;&emsp; <<Exercise 1>> 각 문장을 kkma 분석기로 형태소 단위로 분리.\n",
    "\n",
    "</br> &emsp;&emsp; 형태소로 분석된 문장들과 vocab들을 sonagi.txt, sonagi.voc 파일로 작성.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkaozFjwLqAF"
   },
   "outputs": [],
   "source": [
    "# 4. nltk import하고 sonagi.doc을 읽어들임. nltk의 sent_tokenize를 이용하여 sonagi.doc의 내용을 문장 단위로 분리\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "lines = []\n",
    "with open('sonagi.doc', 'rt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        lines.append(line)\n",
    "\n",
    "sentences = []\n",
    "for line in lines:\n",
    "    sentences.extend(sent_tokenize(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q6MrcQOMNTM-",
    "outputId": "66b41116-1c0e-4d1c-b997-0a4e0b3e5ebd"
   },
   "outputs": [],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kcpOrgRCO-IN",
    "outputId": "51ad75e5-4127-448e-e475-209193137439"
   },
   "outputs": [],
   "source": [
    "tokenizer.morphs(\"이 문장이 형태소 단위로 잘 출력되나요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVmrImWNLu6K"
   },
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "# 5. <<Exercise 1>> 주어진 sentences를 이용하여, sentences를 각 문장별로 형태소 분석한 morph_sentences를 implementation 해주세요.\n",
    "\n",
    "# sentences: ['소년은 개울가에서 소녀를 보자 곧 윤 초시네 증손녀(曾孫女)딸이라는 걸 알 수 있었다.',\n",
    "#             '소녀는 개울에다 손을 잠그고 물장난을 하고 있는 것이다.',\n",
    "#             ... ]\n",
    "#########################################################################################################################\n",
    "\n",
    "\n",
    "morph_sentences = []\n",
    "for sentence in sentences:\n",
    "    temp = \"\"\n",
    "    morphs = tokenizer.morphs(sentence)\n",
    "    for morph in morphs:\n",
    "      temp = temp + \" \" + morph\n",
    "\n",
    "    morph_sentences.append(temp.strip())\n",
    "\n",
    "\n",
    "# <expected results>\n",
    "# morph_sentences: ['소년 은 개울가 에서 소녀 를 보 자 곧 윤 초시 네 증손녀 ( 曾孫女 ) 딸 이 라는 것 을 알 ㄹ 수 있 었 다 .',\n",
    "#                   '소녀 는 개울 에다 손 을 잠그 고 물장난 을 하 고 있 는 것 이 다 .',\n",
    "#                   ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_EZOns9Immuw",
    "outputId": "3104cac8-8819-4c0d-8980-3ad5c6abd554"
   },
   "outputs": [],
   "source": [
    "# 6. 형태소 분석된 결과물 확인\n",
    "\n",
    "print(sentences[0])\n",
    "print(morph_sentences[0])\n",
    "print()\n",
    "print(sentences[1])\n",
    "print(morph_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KysxZCltLwC_"
   },
   "outputs": [],
   "source": [
    "# 7. 형태소 분석된 각 sentence들을 한 줄에 한 문장씩 sonagi.txt 파일로 작성\n",
    "\n",
    "with open('sonagi.txt', 'wt', encoding='utf-8') as f:\n",
    "    for line in morph_sentences:\n",
    "        f.write(line.strip())\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19N303W_LxBJ",
    "outputId": "d81560b9-7177-4f2e-b151-eaa00fc3817c"
   },
   "outputs": [],
   "source": [
    "# 8. 형태소 분석된 각 sentence의 형태소들을 전부 set에 집어넣어 중복을 삭제, set에 들어있는 vocab을 한 줄에 한 형태소씩 .voc 파일로 작성\n",
    "\n",
    "vocab = set()\n",
    "for sentence in morph_sentences:\n",
    "    temp = sentence.split()\n",
    "    for t in temp:\n",
    "        vocab.add(t)\n",
    "\n",
    "print(vocab)\n",
    "\n",
    "with open('sonagi.voc', 'wt', encoding='utf-8') as f:\n",
    "    for line in vocab:\n",
    "        f.write(line.strip())\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "au2RwXvElH4T",
    "outputId": "efa3f900-e89c-4da2-c629-9e34d15af7bb"
   },
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h63hHQfLy6xp"
   },
   "source": [
    "## Step 2: Language model 생성\n",
    "\n",
    "\n",
    "### &emsp;>> ! kenlm/build/bin/lmplz -o [N] < [txt file] > [model file]\n",
    "&emsp;&emsp;[N] : 사용할 모델의 n-gram\n",
    "\n",
    "</br> &emsp;&emsp;[txt file] : 학습(counting)에 사용할 문서 경로\n",
    "\n",
    "</br> &emsp;&emsp;[model file] : 생성할 모델 파일명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3g_yzHeLzbs",
    "outputId": "c7dad09d-2bc1-43ff-cb11-8cbd014c6d94"
   },
   "outputs": [],
   "source": [
    "# 9. kenlm 모델 build\n",
    "\n",
    "! kenlm/build/bin/lmplz -o 3 <sonagi.txt> sonagi.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C77ePn8uRcp2",
    "outputId": "ea9259ed-aeba-4564-b48b-2c29313c4c3b"
   },
   "outputs": [],
   "source": [
    "!ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cozxk_yzJI2"
   },
   "source": [
    "## Step 3: Binary file로 Model 변환\n",
    "### &emsp;>> ! kenlm/build/bin/build_binary [model file] [bin model file]\n",
    "\n",
    "&emsp;&emsp;[N] : n-gram\n",
    "\n",
    "</br>&emsp;&emsp;[model file] : 이미 생성된 모델 파일명\n",
    "\n",
    "</br>&emsp;&emsp;[bin model file] : binary로 생성할 모델 파일명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VLk8DGA5L1dQ",
    "outputId": "5b0d3da7-2b61-4024-c12f-08c6d6b87aae"
   },
   "outputs": [],
   "source": [
    "# 10. 생성된 kenlm 모델 파일 binary  file로 변환\n",
    "\n",
    "! kenlm/build/bin/build_binary sonagi.arpa sonagi.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CxDftsRfL1_d",
    "outputId": "46ceb081-2f38-4904-8924-d3911bb630fd"
   },
   "outputs": [],
   "source": [
    "# 11. model 이 제대로 생성 되었는지 확인\n",
    "# ! echo \"원하는 문장\" | kenlm/build/bin/query [(bin) model file]\n",
    "\n",
    "! echo \"소녀 가 소년 을\" | kenlm/build/bin/query sonagi.bin\n",
    "! echo \"소녀 가 소년 을\" | kenlm/build/bin/query sonagi.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUsVg6IhzToE"
   },
   "source": [
    "## Step 4: model을 활용하여 주어진 시퀀스 scoring\n",
    "\n",
    "### &emsp;>> model.score('여름 이 었 다 . ', bos=True, eos=True)\n",
    "### &emsp;>> model.score('여름 이 었 ', bos=True, eos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aohuLOD2L3E-"
   },
   "outputs": [],
   "source": [
    "# 12. 생성한 모델 불러오기\n",
    "import kenlm\n",
    "\n",
    "model_file = 'sonagi.bin' \n",
    "# model_file = 'sonagi.arpa' # arpa 모델도 사용가능하나, binary 파일이 속도가 좀더 빠름.\n",
    "\n",
    "model = kenlm.Model(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pK9QiuqUL4Ao",
    "outputId": "dcc9fc63-802d-44bd-fb44-cd329c1884fe"
   },
   "outputs": [],
   "source": [
    "# 13. 시퀀스 scoring 예시\n",
    "\n",
    "# model.score('주어진 시퀀스', bos=True, eos=True)\n",
    "# bos : 모델이 scoring하도록 주어진 시퀀스 내에서 sentence가 시작했는가\n",
    "# eos : 모델이 scoring하도록 주어진 시퀀스 내에서 sentence가 종결되었는가\n",
    "\n",
    "# 이미 완성한 시퀀스(문장)의 scoring을 위해서는 bos=True eos=True\n",
    "# bos=True로 하면 모델이 시퀀스의 앞에 <s>를 추가하고,\n",
    "# eos=True인 경우 모델이 시퀀스의 뒤에 </s>를 추가하여 score를 계산함\n",
    "\n",
    "#따라서 bos=True, eos=True는 '<s> 원하는 시퀀스 </s>' 의 scoring을 한 것임.\n",
    "\n",
    "\n",
    "#  \"<s> 여름 이 었 다 . </s>\" 를 scoring'\n",
    "eos_score = model.score('여름 이 었 다 .', bos=True, eos=True)\n",
    "print(\"<s> 여름 이 었 다 . </s> :\", eos_score)\n",
    "\n",
    "#  \"<s> 여름 이 었\" 을 scoring\n",
    "no_eos_score = model.score('여름 이 었', bos=True, eos=False)\n",
    "print(\"<s> 여름 이 었 :\", no_eos_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7EcGs1h3xXv"
   },
   "source": [
    "## Step 5: 주어진 시퀀스에서 다음 형태소 예측\n",
    "\n",
    "### &emsp;<< Exercise 2>> '소년 은 소녀 를' 다음에 올 가장 자연스러운 형태소를 찾아주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URqEQ5LkL55H"
   },
   "outputs": [],
   "source": [
    "# 14. vocab list 파일 불러오기\n",
    "\n",
    "vocab_file = 'sonagi.voc'\n",
    "vocab_list = []\n",
    "with open(vocab_file, 'rt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        vocab_list.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LZpoQfp8L6oS",
    "outputId": "78a6374f-64e3-4e1f-8cc7-488b2f4330e2"
   },
   "outputs": [],
   "source": [
    "###########################################################################################################################\n",
    "# 15. << Exercise 2 >> 주어진 시퀀스('소년 은 소녀 를') 다음의 형태소를 예측 : 주어진 시퀀스에 voc list의 형태소 하나를 합쳐서 score를 계산\n",
    "\n",
    "# 주어진 시퀀스에 preprocessing하며 만든 sonagi.voc 파일의 형태소를 하나씩 붙여서 가장 score가 높은 형태소를 찾음\n",
    "# voc = ['가', '만', '사업', ...] 이라면,\n",
    "#'소년 은 소녀 가' 의 점수, '소년 은 소녀 만', '소년 은 소녀 사업 ', ... 등의 score를 계산하고 그 중 가장 높은 score를 지닌 형태소를 찾음\n",
    "############################################################################################################################\n",
    "\n",
    "best_vocab = None\n",
    "best_score = None\n",
    "\n",
    "for vocab in vocab_list:\n",
    "    sequence_cand = \"소년 은 소녀 를 \" + vocab\n",
    "    score_cand = model.score(sequence_cand, bos=True, eos=False)\n",
    "\n",
    "    if best_vocab is None:\n",
    "      best_score = score_cand\n",
    "      best_vocab = vocab\n",
    "\n",
    "    if best_score < score_cand:\n",
    "      best_score = score_cand\n",
    "      best_vocab = vocab\n",
    "\n",
    "sequence_cand = \"소년 은 소녀 를\"\n",
    "score_cand = model.score(sequence_cand, bos=True, eos=True)\n",
    "\n",
    "if best_score < score_cand:\n",
    "  best_score = score_cand\n",
    "  best_vocab = '</s>'\n",
    "\n",
    "print(\"Best vocab: \", best_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdmfWMpe4MhX"
   },
   "source": [
    "## Step 6: 주어진 시퀀스로 시작하는 가장 자연스러운 문장 예측\n",
    "\n",
    "#### </br>&emsp;<< Exercise 3 >> Step 5와 loop문을 사용하여,\n",
    "#### </br> &emsp;'소년 은 소녀 를' 으로 시작하는 가장 자연스러운 문장을 찾아주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iyc4XiqUL8cA"
   },
   "outputs": [],
   "source": [
    "###############################################################################################################################\n",
    "# 16. << Exercise 3 >> 15를 auto-regressive하게 loop 문으로 반복하여 주어진 시퀀스 '소년 은 소녀 를' 로 시작한 문장이 어떻게 끝날지 예측해주세요.\n",
    "\n",
    "# 1) 주어진 시퀀스에서 문장이 종결될 경우의 score를 계산\n",
    "# 2) 주어진 시퀀스에서 문장이 종결되지 않을 경우에 그 다음에 올 가장 자연스러운 형태소와 score를 계산\n",
    "# 3) 만약 1)보다 2)의 score가 더 높다면, 시퀀스에 2)에서 찾아낸 형태소를 추가하고, 다시 1)부터 반복\n",
    "# 4) 만약 1)의 score가 2)의 score 보다 더 높다면, 그대로 시퀀스를 마무리 함\n",
    "###############################################################################################################################\n",
    "\n",
    "def predict(model_path, voc_path):\n",
    "    # load language model with KenLM\n",
    "    model = kenlm.Model(model_path)\n",
    "    print(\"Language Model at \" + model_path + \" loaded.\")\n",
    "\n",
    "    # load vocabulary (or load word list)\n",
    "    voc = []\n",
    "    file_reader = open(voc_path, \"r\", encoding='utf-8')\n",
    "    for line in file_reader:\n",
    "        voc.append(line.strip())\n",
    "    print(\"Vocabulary at \" + voc_path + \" loaded\\n\")\n",
    "\n",
    "    sequence = input('Sequence: ')\n",
    "\n",
    "    print(\"Given sequence: \" + sequence)\n",
    "\n",
    "    # prediction loop\n",
    "    loop_condition = True\n",
    "    while loop_condition:\n",
    "        best_score = None \n",
    "        best_vocab = None \n",
    "\n",
    "        for vocab in voc:\n",
    "            sequence_cand = sequence + \" \" + vocab\n",
    "            sequence_score = model.score(sequence_cand, bos=True, eos=False)\n",
    "\n",
    "            if best_score is None:\n",
    "                best_score = sequence_score\n",
    "                best_vocab = vocab\n",
    "\n",
    "            if best_score < sequence_score:\n",
    "                best_score = sequence_score\n",
    "                best_vocab = vocab\n",
    "\n",
    "        eos_sentence = sequence\n",
    "        eos_score = model.score(eos_sentence, bos=True, eos=True)\n",
    "        \n",
    "        if eos_score < best_score:\n",
    "            sequence = sequence + \" \" + best_vocab\n",
    "      \n",
    "        else:\n",
    "            loop_condition = False\n",
    "\n",
    "\n",
    "    print(\"Result: \" + sequence)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p9M1VzEIL8fw",
    "outputId": "bf393440-0dd9-4e2c-bafd-62a07b008c29"
   },
   "outputs": [],
   "source": [
    "# 17. 16을 실행 \n",
    "\n",
    "model_path = 'sonagi.arpa'\n",
    "voc_path = 'sonagi.voc'\n",
    "predict(model_path, voc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELatjR6K4zzB"
   },
   "source": [
    "## Step 7: 다른 말뭉치로 학습한 모델과 비교\n",
    "\n",
    "#### </br> &emsp;<< Exercise 3 >> Step 1, 2, 3, 4, 6을 참고하여, \n",
    "#### </br> &emsp;bucketwheat_flowers.doc를 사용한 모델을 생성하여 \n",
    "#### </br> &emsp;주어진 시퀀스('소년 은 소녀 를', '개울가 가', '여름 에')로 시작하는 가장 자연스러운 문장들을 찾고, \n",
    "#### </br> &emsp;sonagi.doc를 사용한 경우와 비교해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bOBIYmOhL_dY",
    "outputId": "ee8bf4de-6748-4b1d-99cc-0aa780c90d30"
   },
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "# 18. << Exercise 4 >> : bucketwheat_flowers.doc를 사용한 모델을 생성하여 sonagi.doc를 사용한 경우와 주어진 시퀀스에 대한 output을 비교해주세요.\n",
    "# 주어진 시퀀스: '소년 은 소녀 를', '개울가 가', '여름 에'\n",
    "\n",
    "# sonata.doc를 data preprocessing하여 sonata.txt, sonata.voc를 생성.\n",
    "# sonata.txt, sonata.voc를 이용하여 모델을 생성, load하여서\n",
    "# 같은 문장에 대해서 sonagi corpus를 이용한 경우와 sonata corpus를 이용한 경우에 output이 어떠한지 출력\n",
    "####################################################################################################################\n",
    "\n",
    "# Data preprocessing. 형태소 분하여 buckwheat_flowers.txt, buckwheat_flowers.voc 작성\n",
    "b_lines = []\n",
    "with open('buckwheat_flowers.doc', 'rt', encoding='utf-8') as b_f:\n",
    "    for b_line in b_f:\n",
    "        b_lines.append(b_line)\n",
    "\n",
    "b_sentences = []\n",
    "for line in b_lines:\n",
    "    b_sentences.extend(sent_tokenize(line))\n",
    "\n",
    "b_morph_sentences = []\n",
    "for sentence in b_sentences:\n",
    "    temp = \"\"\n",
    "    for morpheme in tokenizer.morphs(sentence):\n",
    "      temp = temp + \" \" + morpheme\n",
    "    b_morph_sentences.append(temp.strip())\n",
    "\n",
    "with open('buckwheat_flowers.txt', 'wt', encoding='utf-8') as f:\n",
    "    for line in b_morph_sentences:\n",
    "        f.write(line.strip())\n",
    "        f.write('\\n')\n",
    "\n",
    "b_vocab = set()\n",
    "for sentence in b_morph_sentences:\n",
    "    temp = sentence.split()\n",
    "    for t in temp:\n",
    "        b_vocab.add(t)\n",
    "\n",
    "with open('buckwheat_flowers.voc', 'wt', encoding='utf-8') as f:\n",
    "    for line in b_vocab:\n",
    "        f.write(line.strip())\n",
    "        f.write('\\n')\n",
    "\n",
    "\n",
    "# n-gram language model을 buckwheat_flowers로 학습\n",
    "\n",
    "! kenlm/build/bin/lmplz -o 3 <buckwheat_flowers.txt> buckwheat_flowers.arpa\n",
    "! kenlm/build/bin/build_binary buckwheat_flowers.arpa buckwheat_flowers.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IADqh60wtMp2",
    "outputId": "ae38db0b-5e20-42f7-8b7f-a21eadb14f41"
   },
   "outputs": [],
   "source": [
    "# 19. 18을 실행\n",
    "\n",
    "model_path2 = 'buckwheat_flowers.arpa'\n",
    "voc_path2 = 'buckwheat_flowers.voc'\n",
    "predict(model_path2, voc_path2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "practice_konlpy_kenlm.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
